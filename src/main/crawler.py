import urllib
import re
import requests
from bs4 import BeautifulSoup


class Crawler:

    visited_urls = None
    crawl_need_depth = None
    crawl_current_depth = None

    def set(self, crawl_need_depth):
        self.visited_urls = []
        self.crawl_need_depth = crawl_need_depth
        self.crawl_current_depth = 0

    def crawl(self, urls):
        self.increase_crawl_current_depth()
        if self.crawl_current_depth > self.crawl_need_depth:
            print("\n\nâœ… Crawling complete ğŸ‘")
            return self.visited_urls

        print("ğŸ”¦ Now crawling depth is: " + str(self.crawl_current_depth))

        new_urls = []
        for url in urls:
            # 5. âœ… Remove duplicates (íŒŒì‹±í•œ url list ì¤‘ ì¤‘ë³µ ì œê±°)
            if url in self.visited_urls:
                continue

            try:
                html = urllib.request.urlopen(url)
                self.visited_urls.append(url)

                some_new_urls = self.extract_hyperlinks_from_html(html)
                new_urls.extend(some_new_urls)
                print("  âœ… '" + url + "' crawled ğŸ‘")
            except requests.exceptions.MissingSchema as e:
                print("  âš ï¸ Invalid URL: " + str(e))
            except Exception as e:
                print("  âš ï¸ Unknown exception occurred in '" + url + "':")
                print("      " + str(e))

        # 4. âœ… Crawl again from the parsed hyperlinks. (íŒŒì‹±í•œ url web siteë¥¼ ë‹¤ì‹œ ê°€ì ¸ì˜¤ê¸°)
        return self.crawl(new_urls)

    def extract_hyperlinks_from_html(self, html):
        """
            3. âœ… Parse the html contents to extract hyperlinks (<href="...">) (html ì¤‘ hyperlink ë¶€ë¶„ ì¶”ì¶œ)
        """
        soup = BeautifulSoup(html, "html.parser")
        result = soup.findAll('a', attrs={'href': re.compile("^http://")})

        return [url.get('href') for url in result]

    def increase_crawl_current_depth(self):
        self.crawl_current_depth = self.crawl_current_depth + 1
